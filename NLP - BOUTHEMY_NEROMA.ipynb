{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Firstly, we start by importing the text document and format the sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = open('../word_vectors/sample.txt')\n",
    "\n",
    "document = file.read()\n",
    "lines = preprocess(document)\n",
    "\n",
    "def preprocess(document):\n",
    "    \"\"\"\n",
    "    Do the preprocessing of the document such as removing numbers, punctuation,\n",
    "    delimiting sentences, tokenizing words.\n",
    "    :param document: string document\n",
    "    :return: list of lines with every token words.\n",
    "    \"\"\"\n",
    "    document = document.lower()  # Put it to lowercase\n",
    "    document = document.strip()  # White space removal\n",
    "    document = re.sub(r'\\d+', '', document)  # Remove numbers\n",
    "\n",
    "    # Remove all punctuation (we change the breakline in 1 before)\n",
    "    document = document.replace('\\n', '1')\n",
    "    document = re.sub('\\W+', ' ', document)\n",
    "    lines = document.split('1')   # We get the different sentences.\n",
    "    print(\"Preprocessing is finished.\")\n",
    "\n",
    "    return lines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also do the tokenization and the stemming / lemmatization. In this example, we chose to not use them because\n",
    "the evaluation contains many words conjugated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenization(lines, stemming=False, lemmatization=False):\n",
    "    \"\"\"\n",
    "    Do the tokenization of every word, do the lemmatization and stemming\n",
    "    operation as well.\n",
    "\n",
    "    :param lines: A list of strings. Each element represents a sentence.\n",
    "    :return: List of string of words. Each element represent a sentence.\n",
    "    \"\"\"\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "\n",
    "    for i in range(len(lines)):\n",
    "        lines[i] = lines[i].split()  # Tokenize the lines\n",
    "        words_selected = [j for j in lines[i] if j not in stop_words]  # Remove stop words\n",
    "        lines[i] = words_selected\n",
    "        stemmer = PorterStemmer()  # Define the stemming operation\n",
    "        lemmatizer = WordNetLemmatizer()  # Define the lemmatization\n",
    "        for k in range(len(lines[i])):\n",
    "            if stemming:\n",
    "                lines[i][k] = stemmer.stem(lines[i][k])\n",
    "            if lemmatization:\n",
    "                lines[i][k] = lemmatizer.lemmatize(lines[i][k])\n",
    "    print(\"Tokenization is finished.\")\n",
    "    return lines\n",
    "\n",
    "lines = tokenization(lines)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the word embedding, we are using Word2Vec from the Gensim package. It helps to get the vectorial representation of all the token words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Word2Vec(test, size=100, window=5, min_count=1, workers=4)\n",
    "print(\"Training of the Word2Vec is finished.\")\n",
    "model.save(\"word2vec.model\")\n",
    "\n",
    "\n",
    "word2vec_model = Word2Vec.load(\"word2vec.model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal of our project is to try to approach and replace the unknown words with a correct equivalent. \n",
    "Firstly, we have to create a train set which contains many unknown words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_with_unknown(lines):\n",
    "    \"\"\"\n",
    "    Replace random words with the UNKNOW tag, store all the words in a list.\n",
    "    :param lines:\n",
    "    :return: List of all the real words and new lines with tags.\n",
    "    \"\"\"\n",
    "    real_words = []\n",
    "    unk_lines = lines.copy()\n",
    "    for j in range(len(lines)):\n",
    "        line = lines[j]\n",
    "        if len(line) >= 5:  # We want the unknown word in the middle of five words\n",
    "            index = np.random.random_integers(low=2, high=len(line) - 3)  # Take the index of the word to be choosen\n",
    "            real_words.append(line[index])\n",
    "            unk_lines[j][index] = 'unk'  # Replace it with the unknown tag\n",
    "    print(\"Creation of the unknown dataset.\")\n",
    "    return unk_lines, real_words\n",
    "\n",
    "\n",
    "\n",
    "# First of all we will do the replacement with UNK.\n",
    "\n",
    "unk_test, real_words = predict_unk.replace_with_unknown(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then a first approach to predict the unknown words will be to take four words arounds and take the most similar word. We call this method the prediction with nearest neighbours: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prediction_neighbor_similarities(lines_with_unknown, model):\n",
    "    \"\"\"\n",
    "    Return the prevision for each unknown word based on the similarities with the nearest neighbours.\n",
    "    :param lines_with_unknown: list of string lines.\n",
    "    :param model: the model of Word2Vec\n",
    "    :return: list of the predicted words\n",
    "    \"\"\"\n",
    "    predicted_words = []\n",
    "    for i in range(len(lines_with_unknown)):\n",
    "        line = lines_with_unknown[i]\n",
    "        if 'unk' in line:  # If the line contains the word 'unk'\n",
    "            index = line.index('unk')\n",
    "            neighbours_words = [line[i] for i in\n",
    "                                (index - 2, index - 1, index + 1, index + 2)]  # Extract the words around\n",
    "            most_similar = model.most_similar(positive=neighbours_words)[0][0]\n",
    "            predicted_words.append(most_similar)\n",
    "\n",
    "    return predicted_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then apply this to our dataset and evaluate its performance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unk_lines, real_words = predict_unk.replace_with_unknown(lines)\n",
    "\n",
    "# We have now the train set.\n",
    "\n",
    "# Let's try it with the Nearest-Neighbor\n",
    "\n",
    "predicted_words = predict_unk.prediction_neighbor_similarities(unk_lines, word2vec_model)\n",
    "\n",
    "df = pd.DataFrame()\n",
    "\n",
    "df['Real Words'] = real_words\n",
    "df['Nearest Neighbours'] = predicted_words\n",
    "\n",
    "df['Similarity'] = df.apply(lambda row: word2vec_model.wv.similarity(row['Real Words'], row['Nearest Neighbours']),\n",
    "                            axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then had the idea to use the Part-of-Speech (POS) tag enhancement. It says if a word is a verb, noun, etc...\n",
    "\n",
    "We believe that can help to make accurate prediction because suppose we have the sentence:\n",
    "\n",
    "\"I want to unk football and basketball\".\n",
    "\n",
    "Maybe the most similar words will be \"tennis\" or \"sports\". But here, thanks to the context, we know that the unk word is a verb, and then we can decide to replace the unknown word with a verb which is similar.\n",
    "\n",
    "Rather, than trying to enumerate all the grammar rules, we decided to use a Neural Network (LSTM) to capture the structure of the sentence.\n",
    "\n",
    "For example, if we have \"Noun\" \"to\" \"unk\" \"Noun\" \"Noun\" ; the neural network will say that the POS of the \"unk\" word is more likely to be a verb (because there is a \"to\" before...).\n",
    "\n",
    "We use the NLTK package to do an automatic POS tagging with the known words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First we need to create a list of all the POS tags\n",
    "\n",
    "def assign_POS_tag(lines):\n",
    "    \"\"\"\n",
    "    Assign the POS tags to each lines. If the word is not recognized, we use a special tag \"NULL\".\n",
    "    :param lines: list of lines\n",
    "    :return: lines with the POS tag\n",
    "    \"\"\"\n",
    "    lines_POS = lines.copy()\n",
    "    for i in range(len(lines)):\n",
    "        line = lines[i]\n",
    "        association = nltk.pos_tag(line)  # Get the list of tokens with their POS tags\n",
    "        lines_POS[i] = [x[1] for x in association]\n",
    "    print(\"Creation of the POS tags list is finished.\")\n",
    "\n",
    "    return lines_POS\n",
    "\n",
    "lines_POS = assign_POS_tag(lines)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then create a simple NN (with a LSTM layer) to capture the temporal dependency of the sentence.\n",
    "Moreover, we output the density of probability for each category (there are 45 possible POS tag) and the loss function will be the categorical cross entropy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_neural_network():\n",
    "    \"\"\"\n",
    "    Create the LSTM Sequential Neural Network.\n",
    "    :return: A Neural network created with Keras.\n",
    "    \"\"\"\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(32, input_shape=(4, 45)))  # 4 time-steps and 45 features\n",
    "    model.add(Dense(64))\n",
    "    model.add(Activation('tanh'))\n",
    "    model.add(Dense(units=45))  # 45 is the number of class\n",
    "    model.add(Activation('softmax'))  # Output the density of probability\n",
    "\n",
    "    model.compile(optimizer=adam(lr=0.001, decay=1e-6),\n",
    "                  loss=\"categorical_crossentropy\",\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "    model.summary()\n",
    "    print(\"Creation of the Neural Network is finished.\")\n",
    "    return model\n",
    "\n",
    "# We use a neural network (with Keras)\n",
    "\n",
    "NN_model = create_neural_network()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will then create the X and Y to feed the Neural Network, because the NN only takes numbers, we will before do a\n",
    "one-hot encoding to every word.\n",
    "\n",
    "Then we will take a sequence of 4 words to predict the fifth one (which is in the middle of the sentence)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_encoding(POS_tag, list_tags):\n",
    "    \"\"\"\n",
    "    Transform the POS tag to the classification array.\n",
    "    :param POS_tag: a string\n",
    "    :return: the array with one-hot encoding.\n",
    "    \"\"\"\n",
    "    if POS_tag in list_tags:\n",
    "        position = list_tags.index(POS_tag)  # Take the position of the word\n",
    "    else:\n",
    "        position = np.random.randint(len(list_tags) - 1)\n",
    "    class_array = np.zeros(len(list_tags))\n",
    "    class_array[position] = 1  # Assign one to the correct class\n",
    "    return class_array\n",
    "\n",
    "\n",
    "def convert_int_data(lines):\n",
    "    \"\"\"\n",
    "    Convert the lines with tags to array of 0 and 1 for the neural network.\n",
    "    :param lines: the lines with tags indices.\n",
    "    :param tag_list: the list of all possible tags.\n",
    "    :return: X and Y sets ready for the neural network.\n",
    "    \"\"\"\n",
    "    tagdict = load('help/tagsets/upenn_tagset.pickle')\n",
    "    list_tags = list(tagdict.keys())  # Get the list of all the tags.\n",
    "    X, Y = [], []  # Creation of the array\n",
    "    for j in range(len(lines)):\n",
    "        line = lines[j]\n",
    "        if len(line) >= 5:  # We want the word in the middle of five words\n",
    "            index = np.random.random_integers(low=2, high=len(line) - 3)  # Take the index of the word to be choosen\n",
    "            neighbours_words = [line[i] for i in (index - 2, index - 1, index + 1, index + 2)]  # Extract the words\n",
    "            Y.append(one_hot_encoding(lines[j][index], list_tags))  # Append the target to the array\n",
    "            sample = []\n",
    "            for word in neighbours_words:\n",
    "                sample.append(one_hot_encoding(word, list_tags).tolist())\n",
    "            X.append(sample)  # Append the 4 neighbouring words\n",
    "\n",
    "    return np.array(X), np.array(Y)\n",
    "\n",
    "X_train, Y_train = convert_int_data(train_set)\n",
    "X_test, Y_test = convert_int_data(test_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can the fit and train the neural network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NN_model.fit(X_train, Y_train)\n",
    "print(\"Training of the Neural Network is finished.\")\n",
    "Y_pred = NN_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The accuracy is calculated, we compare the class prediction with the real class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_accuracy(Y_test, Y_pred):\n",
    "    \"\"\"\n",
    "    Compute the accuracy between the prediction and the actual classification of the word.\n",
    "    :param Y_test: the real POS tag of the word.\n",
    "    :param Y_pred: the predicted POS tag of the word.\n",
    "    :return: the percentage of correct predictions.\n",
    "    \"\"\"\n",
    "    number_correct_prediction = 0\n",
    "    for i in range(len(Y_pred)):  # They have the same length\n",
    "        id_pred = np.argmax(Y_pred[i])  # Take the argmax of the prediction\n",
    "        id_test = np.where(Y_test[i] == 1.)[0][0]  # Take the real position of the POS tag\n",
    "        if id_test == id_pred:\n",
    "            number_correct_prediction += 1\n",
    "\n",
    "    percentage_correct = number_correct_prediction / len(Y_pred)\n",
    "\n",
    "    return percentage_correct\n",
    "\n",
    "accuracy = compute_accuracy(Y_test, Y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We believe that the accuracy is not quite good because we have included too many categories (45) where we can restrict to less. Moreover, the lack of data and the structure of the neural network (architecture and hyperparameters) might be not the most appropriate. \n",
    "\n",
    "However, we believe that this method is quite useful and can lead to good results if correctly trained."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then do the prediction with neighbours but with the inclusion of POS tag.\n",
    "We select the top 10 most similars words and then we check if the POS of one of them correspond to the POS predicted by the LSTM network. If so, we select that word, otherwise, we select the one with the most important similarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prediction_neighbor_with_pos(lines_with_unknown, word2vec_model, NN_model):\n",
    "    \"\"\"\n",
    "    Return the prevision for each unknown word based on the similarities with the nearest neighbours.\n",
    "    This time, we used the lstm pos model to select the word based on its POS tag.\n",
    "    :param lines_with_unknown: list of string lines with token words.\n",
    "    :param word2vec_model: the word2vec model that we used to get similar words.\n",
    "    :param lstm_model: we use this one to predict the pos tag of the word.\n",
    "    :return: predicted words.\n",
    "    \"\"\"\n",
    "    predicted_words = []  # List of all the predicted words\n",
    "    tagdict = load('help/tagsets/upenn_tagset.pickle')\n",
    "    list_tags = list(tagdict.keys())  # Get the list of all the tags.\n",
    "    for i in range(len(lines_with_unknown)):\n",
    "        line = lines_with_unknown[i]\n",
    "        if 'unk' in line:  # If the line contains the word 'unk'\n",
    "            index = line.index('unk')\n",
    "            neighbours_words = [line[i] for i in\n",
    "                                (index - 2, index - 1, index + 1, index + 2)]  # Extract the words around\n",
    "            most_similar_list = word2vec_model.most_similar(positive=neighbours_words)[:10]\n",
    "            sample = []\n",
    "\n",
    "            for word in neighbours_words:  # Format the neighbouring words for the Neural Network\n",
    "                sample.append(one_hot_encoding(word, list_tags).tolist())\n",
    "\n",
    "            Y_pos = NN_model.predict(np.array(sample).reshape((1, 4, 45)))  # Predict the vector of POS tag\n",
    "            id_pos = np.argmax(Y_pos)  # Take the id\n",
    "            pos_tag = list_tags[\n",
    "                id_pos]  # We got now the POS tag which is predicted, we can get a more accurate prediction\n",
    "\n",
    "            # We then check if there is a word if the corresponding POS tag among the top 10,\n",
    "\n",
    "            best_candidate = []\n",
    "            for i in range(len(most_similar_list)):\n",
    "                word = most_similar_list[i][0]\n",
    "                if nltk.pos_tag([word]) == pos_tag:\n",
    "                    best_candidate.append(word)\n",
    "\n",
    "            if best_candidate:  # If the list is not empty\n",
    "                predicted_words.append(best_candidate[0])  # Take the first element\n",
    "            else:\n",
    "                predicted_words.append(most_similar_list[0][0])  # Otherwise we just take the first element\n",
    "\n",
    "    return predicted_words\n",
    "\n",
    "df['LSTM enhancement'] = predict_unk.prediction_neighbor_with_pos(unk_test, word2vec_model, NN_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then apply this method on the unknown list, to tag the unknown words and try to detect them.\n",
    "\n",
    "Finally, we calculate the similarity (and Spearman ranking) and the analogy score on the two dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def spearman(model):\n",
    "    \"\"\"\n",
    "    Calculate the similarities between each words and perform the Spearman ranking.\n",
    "    \"\"\"\n",
    "    df_sim = pd.read_csv('../word_vectors/wordsim353.csv')\n",
    "\n",
    "    # Apply the similarity function to each row\n",
    "    df_test = df_sim.copy()  # Test dataframe\n",
    "    df_test['Similarity'] = df_test.apply(lambda row: model.wv.similarity(row['Word 1'], row['Word 2']),\n",
    "                                          axis=1)\n",
    "\n",
    "    spearman_rank = spearmanr(df_test['Human (mean)'], df_test['Similarity']).correlation\n",
    "    df_test['Spearman'] = spearman_rank\n",
    "    print(\"Spearman ranking between similarities is finished. Value is \" + str(spearman_rank))\n",
    "\n",
    "    return df_test\n",
    "\n",
    "\n",
    "def analogy(model):\n",
    "    \"\"\"\n",
    "    Get the analogy for each phrase.\n",
    "    :param model: The Word2Vec model which has been trained.\n",
    "    :return:\n",
    "    \"\"\"\n",
    "\n",
    "    df_analogy = pd.read_csv('../word_vectors/questions-words.txt', sep=\" \", header=None, skiprows=1)\n",
    "    df_analogy = df_analogy.dropna()  # Drop rows with NaN\n",
    "    df_analogy.columns = ['Word 1', 'Analogy 1', 'Word 2', 'Analogy 2']\n",
    "    df_analogy = df_analogy.applymap(lambda s: s.lower() if type(s) == str else s)  # Convert to lowercase\n",
    "\n",
    "    df_test = df_analogy.copy()  # Get the test dataframe\n",
    "\n",
    "    # For each line, we find the analogy and we write in the column prediction.\n",
    "    df_test['Prediction'] = df_test.apply(lambda row: model.most_similar(positive=[row['Word 2'], row['Analogy 1']],\n",
    "                                                                         negative=[row['Word 1']])[0][0], axis=1)\n",
    "    print(\"Computation of the analogies is finished.\")\n",
    "\n",
    "    return df_test\n",
    "\n",
    "\n",
    "df_sim = spearman(model)\n",
    "\n",
    "df_analogy = analogy(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Marin B0UTHEMY & Kossi NEROMA, ENSAE 3A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
